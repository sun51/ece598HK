\documentclass[11pt,onecolumn]{article}
\usepackage{makeidx,times,alltt,graphicx,calc,subfigure}
\usepackage{epstopdf}
\usepackage{xspace}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{fancyvrb}
\usepackage{booktabs}
\usepackage{ctable}
\usepackage{multirow}
\usepackage{bigdelim}
\usepackage{ctable}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage{listings}
\usepackage{epsfig,alltt}
\usepackage{graphics}
\usepackage{shortvrb}
%\usepackage[english,ruled,vlined]{algorithm2e}
%\usepackage{latexsym}
%\usepackage{amssymb,amsmath}
%\usepackage[usenames]{color}
\usepackage{setspace}
\usepackage{comment}
%\usepackage{ulem}

\usepackage{listings}
\lstset{fancyvrb=true}
\lstset{
    language=C++,
        basicstyle=\footnotesize\tt,
        %basicstyle=\tt, 
        %keywordstyle=\color{blue},
        identifierstyle=,
        %commentstyle=\color{green},
        %stringstyle=\color{red},
        showstringspaces=false,
        captionpos=t,
        tabsize=3,
        %linewidth=12mm,
        numbers=left, 
        stepnumber=2
}

%\usepackage{pdftricks}
%\begin{psinputs}
%\usepackage{pstricks}
%\usepackage{color}
%\usepackage{pstcol}
%\usepackage{pst-plot}
%\usepackage{pst-tree}
%\usepackage{pst-eps}
%\usepackage{multido}
%\usepackage{pst-node}
%\usepackage{pst-eps}
%\end{psinputs}

%\newcommand{\comment}[1]{}
\newcommand{\charmpp}{\textsc{Charm++}}
\newcommand{\namd}{\textsc{namd}}
\newcommand{\charisma}{Charisma}
\newcommand{\divcon}{\emph{DivCon}}
\newcommand{\openatom}{\textsc{OpenAtom}}
\newcommand{\LeanCP}{\textsc{OpenAtom}}
\newcommand{\leancp}{\textsc{OpenAtom}}
\newcommand{\changa}{ChaNGa}
\newcommand{\kale}{Kale}
\newcommand{\sdag}{Structured Dagger}
\newcommand{\parfum}{{\sc ParFUM}}
\newcommand{\metis}{{\sc Metis}}
\newcommand{\note}[1]{\emph{(Note: #1)}}
\newcommand{\tight}{\baselineskip=8pt}
\newcommand{\etal}{{\em et al.}}
\newcommand{\viz}{{\em viz.}}
\newcommand{\nbody}{$N$-body}
\newcommand{\kdtree}{$k$d-tree}

\def\code#1{{\small {\tt {#1}}}}
\def\smallfbox#1{{\small {\fbox{#1}}}}
\def\porder#1#2#3{$#1 <_{#3} #2$}
\def\lhs#1#2{$#2 \in \mathit{LHS}(#1)$}
\def\rhs#1#2{$#2 \in \mathit{RHS}(#1)$}


\oddsidemargin=-0.25in
\textwidth=7in
\topmargin=-0.25in
\headheight=0in
\headsep=0in
\textheight=9.5in

\begin{document}
%\doublespacing

\title{ ECE598KH Final Project Report \\ NAMD GPU Performance Analysis and Tuning}

\author{
  Yanhua Sun, Xin Zhao\\
  University of Illinois at Urbana-Champaign\\
  \{sun51, xinzhao3\}@illinois.edu
}

\date{}
\maketitle

\lstset{
  basicstyle=\ttfamily,
  showstringspaces=false
}

%\begin{tight}
%\bibliographystyle{abbrv}
%\bibliography{group,cited}
%\end{tight}

\section{Introduction}
In this project, the application we are focusing on is NAMD.
NAMD~\cite{NamdSC02}, recipient of a 2002 Gordon Bell Award, is a parallel molecular 
dynamics code designed for high-performance simulation of large biomolecular systems.
It was developed in the mid-1990's. Now it is one of the most widely used molecular dynamics 
software with more than 50,000 users. It was also selected by NSF as an acceptance test
for Blue Waters.
Through many years, NAMD has been highly optimized to achieve scalable performance on CPU.
It has successfully scaled to the full Titan supercomputer with around $300K$ cores and 
the full Blue Waters with $400K$ cores.

These years, GPU-accelerated algorithms have demostrated speedup of 10- to 100-fold. Meanwhile, 
the hardware has been improved significantly to better match the needs of algorithms over the years.
Since 2006, NAMD has been accommodated to take the advantage of GPU-accelerators. Speedup of 
6- or even 10- has been demonstrated in the previous work~\cite{phillips_stone_namd_cuda}.
Although NAMD GPU has been carefully designed and optimized, it still needs a lot of 
efforts to maximize the performance on GPU, especially on the new hardware such as Fermi, Kepler. 

In this report, we first analyzed NAMD performance on two machines with different configurations.
One is with fast CPU but slow old GPU while the other has fast CPU but slower CPU. We showed 
the results and explained the reasons. Based on our analysis, we optimized the GPU kernel code
to reduce the GPU time. This optimization helps when GPU becomes performance bottleneck.
At the same time, due to the difference of total CPU and GPU capabilities, the load between them
can be imbalanced. We also proposed and implemented a load balancing strategy between CPU and GPU.
We demostrate the effectiveness of our optimizations by comparing the original performance results
and new results.

%\namd{}

\section{NAMD GPU Design } 
\input{design}

\section{NAMD GPU CPU Load Balance}
\label{sec:balance}
\input{loadbalance}
%\section{Background}
%In a molecular dynamics simulation, a collection of atoms interact through a set of forces. 
%Since each atom interacts with all the other atoms, the complexity of calculating forces
%is $O(Atoms^2)$. This algorithm does not scale with the number of atoms. Based on the fact that
%atoms from far distance contribute little to the force calculation, in \namd{} the interaction is
%calculated by short range forces and long range forces. The molecule  is spatially divided into 
%patches based on cutoff distance. Atoms in one patch only interact with other atoms in neighbor patches.
%To be more accurate, long range electrostatics calculation is performed through particle-mesh Ewald (PME) method.
%For these two types of calculation, short range forces are computation intensive while PME is communication intensive.
%Therefore, in current NAMD GPU design and implementation, non-bonded cutoff computation is offload to GPU.
%All other bonded work, integration and PME work is remained on CPU.

%\section{Objective}

\section{NAMD GPU Kernel Optimization}
\input{gpu-opt}

\section{Performance Results}
\input{perf-result}

\section{Conclusion and Discussion}
\input{conclusion}

\bibliographystyle{abbrv}
\bibliography{group,cited}
\end{document}


