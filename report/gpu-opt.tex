\subsection{GPU Force Calculation Kernel}

In the original GPU kernel implementation, each thread block calculates the forces on the atoms in one patch (patch 1)
due to the atoms in another patch (patch 2). Fig.~\ref{figs:pseudocode} presents the pseudo-code for kernel calculation.
In the outer loop, each thread copies atoms from patch 1 to local registers. In the inner loop. threads in the block collaborate to
load atoms from patch 2 to shared memory. Before and after reading of atoms in patch 2, there are synchronization calls to guarantee
that data is ready to be used before the calculation and is ready to be rewritten after the calculation. During the force
calculation phase, each thread iterates atoms in patch 2 and computes the distance with current atom in patch 1.
If the distance is within cut-off distance, it accumulates the forces for the current atom in patch 1 and finally writes force results
back to GPU global memory. The kernel also calculate a pairlist every 10 time steps used by following time steps to further avoid redundant work. 

\begin{figure}[h]
\centering
\setlength{\abovecaptionskip}{-1pt}
\setlength{\belowcaptionskip}{-2pt}
\includegraphics[width=4.0in]{figs/pseudocode.eps}
\caption{Pseudocode for GPU kernel force calculation}
\label{figs:pseudocode}
%\vspace{-0.5cm}
\end{figure}

After profiling the performance of the GPU kernel, we acquired the following observations:
(1) force calculation dominates the most GPU time compared to data loads and stores;
(2) there are great control divergence among different threads due to thread synchronization, which means some threads take a really long time
to reach the synchronization where other threads reach it very quickly. However, all threads in the block have to wait for everyone to
reach the synchronization before they can process further. Besides, use of pairlist may aggravate the divergence.
Based on above observations, we proposed several optimization approaches to reduce the control
divergence. They are described in detail in following sections.

\subsection{Optimizations}
\subsubsection{Patch 1 Atoms Tiling}

As indicated in Fig.~\ref{figs:pseudocode}, in each outer loop, for one atom in patch 1, every thread needs to iterate and examine all loaded atoms from patch 2
and call synchronization routines. In order to reduce the synchronization overhead, we first considered tiling of patch 1 atoms,
in which every thread loads mulitple atoms instead of only one atom from patch 1 in each outer loop.
By doing this, total number of outer loops can be reduced. Furthermore, data reuse of patch 2 is improved because now atoms from patch 2 is used to calculate
forces of more atoms from patch 1.

Fig.~\ref{figs:divergence} shows an example of how merging loops can improve the performance. Suppose thread 1 (T1) has 9 units of computation in the first loop and 2 units
of computation in the second loop, whereas thread 2 (T2) has 3 units to do in the first loop and 8 units to do in the second loop. In the first loop T2 needs to wait for
completion of T1 while in the second loop T1 needs to wait for T2. Both threads have to spend long time waiting for each other and total synchronization time is 17.
After merging loops on both T1 and T2, divergence between them is reduced, synchronization call is decreased by 1, and now total synchronization time is 11.

\begin{figure}[h]
\centering
\setlength{\abovecaptionskip}{-1pt}
\setlength{\belowcaptionskip}{-5pt}
\includegraphics[width=6.0in]{figs/divergence.eps}
\caption{Reduce control divergence by merging loops}
\label{figs:divergence}
\vspace{-0.5cm}
\end{figure}

\subsubsection{Patch 2 Atoms Tiling}

After finishing tiling of loading patch 1 atoms, we consider tiling of loading patch 2 atoms to further reduce synchronization calls.
In the original implementation in Fig.~\ref{figs:pseudocode}, for each inner loop, all threads in the thread block collaborate to load SHARED\_SIZE
patch 2 atoms for just one time.
In our optimization, we make all threads load patch 2 atoms multiple times with SHARED\_SIZE in each time. Fig.~\ref{figs:pseudocode-tile} shows
the pseudo-code after tiling is applied. TILE\_WIDTH\_1 and TILE\_WIDTH\_2 are macros to set tile widths for patch 1 and patch 2 correspondingly.

\begin{figure}[h]
\centering
\setlength{\abovecaptionskip}{-1pt}
\setlength{\belowcaptionskip}{-2pt}
\includegraphics[width=4.0in]{figs/pseudocode-tile.eps}
\caption{Pseudocode for GPU kernel force calculation after tiling optimization}
\label{figs:pseudocode-tile}
%\vspace{-0.5cm}
\end{figure}

\subsubsection{Patch 1 Atoms Sorting}
